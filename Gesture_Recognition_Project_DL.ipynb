{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Problem Statement](#Problem_Statement)\n",
    "- [Generator](#Generator)\n",
    "- [Models](#Model)\n",
    "    - Conv3D:\n",
    "    -- [Model 1: No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10](#Model_1)\n",
    "    -- [Model 2: No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6](#Model_2)\n",
    "    -- [Model 3: No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10](#Model_3)\n",
    "    -- [Model 4: No of Epochs = 25 , batch_size = 50 ,shape = (120,120) , no of frames = 10](#Model_4)\n",
    "    -- [Model 5: No of Epochs = 25 , batch_size = 50 ,shape = (70,70) , no of frames = 18](#Model_5)\n",
    "    - CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
    "    -- [Model 6: No of Epochs = 25 , batch_size = 50 ,shape = (70,70), no of frames = 18](#Model_6)\n",
    "    - Transfer Learning Using MobileNet\n",
    "    -- [Model 7:  No of epochs = 15; batch_size = 5; shape (120,120); no of frames = 18](#Model_9)\n",
    "- [Conclusion](#Conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Introduction\">Introduction</a></h2>\n",
    "\n",
    "In this group project, we are going to build a different model that will be able to predict the 5 gestures correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Problem_Statement\">Problem Statement</a></h2>\n",
    "\n",
    "    - We want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
    "    - The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "        -- Thumbs up:  Increase the volume\n",
    "        -- Thumbs down: Decrease the volume\n",
    "        -- Left swipe: 'Jump' backwards 10 seconds\n",
    "        -- Right swipe: 'Jump' forward 10 seconds  \n",
    "        -- Stop: Pause the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following libraries to get started.\n",
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread, imresize\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Generator\">Generator</a></h2>\n",
    "\n",
    "This is one of the most important parts of the code. In the generator, we are going to pre-process the images as we have images of different dimensions (50 x 50, 70 x 70 and 120 x 120) as well as create a batch of video frames. The generator should be able to take a batch of videos as input without any error. Steps like cropping/resizing and normalization should be performed successfully.  We have to experiment with `img_idx`, `y`,`z` and normalization such that we get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#!pip install scikit-image\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #img_idx = #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        #Shuffle the list of the folders in csv\n",
    "        t = np.random.permutation(folder_list)\n",
    "         #Exact batches of the batch size\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "         #Left over batches which should be handled separately\n",
    "        leftover_batches = len(t) - num_batches * batch_size\n",
    "        \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                   \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    image = resize(image, (shape_h,shape_w))\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
    "\n",
    "                #Fill the one hot encoding stuff where we maintain the label\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if leftover_batches != 0:\n",
    "            for batch in range(num_batches): \n",
    "                # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "                batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) \n",
    "                # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
    "                batch_labels = np.zeros((batch_size,5)) \n",
    "                for folder in range(batch_size): # iterate over the batch_size\n",
    "                    imgs = os.listdir(source_path +'/'+t[batch * batch_size + folder].split(';')[0])\n",
    "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                        \n",
    "                        image = imageio.imread(source_path +'/'+t[batch * batch_size + folder].split(';')[0] +'/'+imgs[item]).astype(np.float32)\n",
    "                        image = resize(image, (shape_h,shape_w))\n",
    "\n",
    "                        batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
    "                        batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
    "                        batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
    "                        \n",
    "                    #Fill the one hot encoding stuff where we maintain the label\n",
    "                    batch_labels[folder, int(t[batch * batch_size + folder].split(';')[2])] = 1\n",
    "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video is represented above in the generator as (number of images, height, width, number of channels). We take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/datasets/Project_data/train'\n",
    "val_path = '/datasets/Project_data/val'\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model\">Model</a></h2>\n",
    "\n",
    "Here we make the model using different functionalities that Keras provides. We must use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. We would also use `TimeDistributed` while building a Conv2D + RNN model. Also, the last layer is the softmax. We design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,  Dropout, LSTM, ConvLSTM2D\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "#write your model here\n",
    "class Conv3DModel():\n",
    "    \n",
    "    def Model3D(self,frames_to_sample,image_height,image_width):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(frames_to_sample,image_height,image_width,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "        model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        # model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        # model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation='elu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        \n",
    "        #write your optimizer TRY OUT WITH ADAM AND SGD\n",
    "        '''\n",
    "        Classes\n",
    "        class Adadelta: Optimizer that implements the Adadelta algorithm.\n",
    "\n",
    "        class Adagrad: Optimizer that implements the Adagrad algorithm.\n",
    "\n",
    "        class Adam: Optimizer that implements the Adam algorithm.\n",
    "\n",
    "        class Adamax: Optimizer that implements the Adamax algorithm.\n",
    "\n",
    "        class Ftrl: Optimizer that implements the FTRL algorithm.\n",
    "\n",
    "        class Nadam: Optimizer that implements the NAdam algorithm.\n",
    "\n",
    "        class Optimizer: Base class for Keras optimizers.\n",
    "\n",
    "        class RMSprop: Optimizer that implements the RMSprop algorithm.\n",
    "\n",
    "        class SGD: Gradient descent (with momentum) optimizer.\n",
    "        '''\n",
    "        \n",
    "        optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have written the model, the next step is to `compile` the model. When we print the `summary` of the model, we can see the total number of parameters we have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global vars\n",
    "def global_vars(img_idx,shape_h,shape_w,batch_size,num_epochs):\n",
    "    print(\"the number of images we will be feeding in the input for a video {}\".format(len(img_idx)))\n",
    "    return img_idx,shape_h,shape_w,batch_size,num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_1\">Model 1:</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 10, 120, 120, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 120, 120, 64)  256      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10, 120, 120, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 5, 60, 120, 64)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 5, 60, 120, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 60, 120, 128)  512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5, 60, 120, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 30, 60, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 30, 60, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 30, 60, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 30, 60, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 15, 30, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 15, 30, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 15, 30, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 15, 30, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 8, 15, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30720)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30720)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               15729152  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,615,813\n",
      "Trainable params: 18,614,405\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 06:31:17.417172: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-11-11 06:31:17.417262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1d:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Model 1: No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([6,8,10,12,14,16,20,22,24,26],120,120,64,15)\n",
    "conv_model1=Conv3DModel()\n",
    "conv_model1=conv_model1.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "#Fix the file path        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "#Callback to save the Keras model or model weights at some frequency.\n",
    "#ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights.\n",
    "#path to save the model file.\n",
    "#\"val_loss\" to monitor the model's total loss in validation.\n",
    "#saves when the model is considered the \"best\"\n",
    "#the model's weights will be saved\n",
    "#the minimization of the monitored quantity\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Reduce learning rate when a metric has stopped improving.\n",
    "#LR = ReduceLROnPlateau(monitor, factor, aptience, min_lr)\n",
    "#monitor: quantity to be monitored.\n",
    "#factor: factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "#patience: number of epochs with no improvement after which learning rate will be reduced.\n",
    "#min_lr: lower bound on the learning rate.\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=6 )\n",
    "# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 06:31:35.093247: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n",
      "2024-11-11 06:31:37.939681: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2024-11-11 06:31:38.863284: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2024-11-11 06:31:39.836283: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2024-11-11 06:31:51.285408: W tensorflow/core/common_runtime/bfc_allocator.cc:463] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.10GiB (rounded to 1179648000)requested by op gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-11-11 06:31:51.285496: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2024-11-11 06:31:51.285514: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 49, Chunks in use: 47. 12.2KiB allocated for chunks. 11.8KiB in use in bin. 2.7KiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285526: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 11, Chunks in use: 10. 5.8KiB allocated for chunks. 5.0KiB in use in bin. 5.0KiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285537: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 18, Chunks in use: 17. 18.8KiB allocated for chunks. 17.2KiB in use in bin. 17.0KiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285546: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 3, Chunks in use: 2. 6.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285556: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285566: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 20.0KiB allocated for chunks. 20.0KiB in use in bin. 20.0KiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285576: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 2, Chunks in use: 2. 40.5KiB allocated for chunks. 40.5KiB in use in bin. 40.5KiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285585: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285594: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285603: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285611: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285621: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 2, Chunks in use: 2. 1.69MiB allocated for chunks. 1.69MiB in use in bin. 1.69MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285630: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 1.05MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285639: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 3, Chunks in use: 2. 9.91MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285650: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 2, Chunks in use: 1. 11.38MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285660: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 12.62MiB allocated for chunks. 12.62MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285670: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 0. 25.25MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285680: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 1. 60.00MiB allocated for chunks. 60.00MiB in use in bin. 60.00MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285697: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 1. 237.47MiB allocated for chunks. 68.00MiB in use in bin. 60.00MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285708: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 3, Chunks in use: 3. 488.03MiB allocated for chunks. 488.03MiB in use in bin. 442.97MiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285719: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 12, Chunks in use: 9. 13.63GiB allocated for chunks. 12.08GiB in use in bin. 12.08GiB client-requested in use in bin.\n",
      "2024-11-11 06:31:51.285729: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 1.10GiB was 256.00MiB, Chunk State: \n",
      "2024-11-11 06:31:51.285746: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 383.50MiB | Requested Size: 1.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 168.75MiB | Requested Size: 168.75MiB | in_use: 1 | bin_num: -1\n",
      "2024-11-11 06:31:51.285757: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 473.62MiB | Requested Size: 168.75MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.10GiB | Requested Size: 1.10GiB | in_use: 1 | bin_num: -1\n",
      "2024-11-11 06:31:51.285768: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 721.00MiB | Requested Size: 337.50MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.10GiB | Requested Size: 1.10GiB | in_use: 1 | bin_num: -1\n",
      "2024-11-11 06:31:51.285776: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 6394871808\n",
      "2024-11-11 06:31:51.285789: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efbbc000000 of size 2359296000 next 100\n",
      "2024-11-11 06:31:51.285797: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efc48a00000 of size 1179648000 next 104\n",
      "2024-11-11 06:31:51.285804: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efc8ef00000 of size 1179648000 next 21\n",
      "2024-11-11 06:31:51.285812: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efcd5400000 of size 1179648000 next 112\n",
      "2024-11-11 06:31:51.285819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efd1b900000 of size 496631808 next 18446744073709551615\n",
      "2024-11-11 06:31:51.285827: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4294967296\n",
      "2024-11-11 06:31:51.285835: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efd98000000 of size 2359296000 next 95\n",
      "2024-11-11 06:31:51.285842: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efe24a00000 of size 589824000 next 101\n",
      "2024-11-11 06:31:51.285850: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efe47c80000 of size 589824000 next 102\n",
      "2024-11-11 06:31:51.285857: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efe6af00000 of size 176947200 next 108\n",
      "2024-11-11 06:31:51.285865: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efe757c0000 of size 176947200 next 116\n",
      "2024-11-11 06:31:51.285872: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efe80080000 of size 402128896 next 18446744073709551615\n",
      "2024-11-11 06:31:51.285880: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4294967296\n",
      "2024-11-11 06:31:51.285887: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7eff18000000 of size 2359296000 next 94\n",
      "2024-11-11 06:31:51.285894: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7effa4a00000 of size 1179648000 next 103\n",
      "2024-11-11 06:31:51.285902: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7effeaf00000 of size 756023296 next 18446744073709551615\n",
      "2024-11-11 06:31:51.285909: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 268435456\n",
      "2024-11-11 06:31:51.286479: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0090000000 of size 110592000 next 89\n",
      "2024-11-11 06:31:51.286488: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0096978000 of size 157843456 next 18446744073709551615\n",
      "2024-11-11 06:31:51.286495: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 134217728\n",
      "2024-11-11 06:31:51.286503: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f00bc000000 of size 62914560 next 47\n",
      "2024-11-11 06:31:51.286511: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f00bfc00000 of size 71303168 next 18446744073709551615\n",
      "2024-11-11 06:31:51.286519: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 67108864\n",
      "2024-11-11 06:31:51.286527: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f00c4000000 of size 67108864 next 18446744073709551615\n",
      "2024-11-11 06:31:51.286534: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 33554432\n",
      "2024-11-11 06:31:51.286541: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f00e0000000 of size 7077888 next 36\n",
      "2024-11-11 06:31:51.286549: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f00e06c0000 of size 26476544 next 18446744073709551615\n",
      "2024-11-11 06:31:51.286556: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 8388608\n",
      "2024-11-11 06:31:51.286564: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f00ec000000 of size 3538944 next 69\n",
      "2024-11-11 06:31:51.286571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f00ec360000 of size 4849664 next 18446744073709551615\n",
      "2024-11-11 06:31:51.286579: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 16777216\n",
      "2024-11-11 06:31:51.286586: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f00ec800000 of size 3538944 next 28\n",
      "2024-11-11 06:31:51.286594: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f00ecb60000 of size 13238272 next 18446744073709551615\n",
      "2024-11-11 06:31:51.286601: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2097152\n",
      "2024-11-11 06:31:51.286609: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800000 of size 256 next 1\n",
      "2024-11-11 06:31:51.286616: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800100 of size 1280 next 2\n",
      "2024-11-11 06:31:51.286623: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800600 of size 256 next 3\n",
      "2024-11-11 06:31:51.286631: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800700 of size 256 next 4\n",
      "2024-11-11 06:31:51.286638: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800800 of size 256 next 5\n",
      "2024-11-11 06:31:51.286645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800900 of size 256 next 6\n",
      "2024-11-11 06:31:51.286652: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800a00 of size 256 next 9\n",
      "2024-11-11 06:31:51.286659: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800b00 of size 256 next 10\n",
      "2024-11-11 06:31:51.286667: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800c00 of size 256 next 11\n",
      "2024-11-11 06:31:51.286675: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800d00 of size 512 next 13\n",
      "2024-11-11 06:31:51.286682: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137800f00 of size 256 next 14\n",
      "2024-11-11 06:31:51.286690: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801000 of size 256 next 15\n",
      "2024-11-11 06:31:51.286697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801100 of size 512 next 16\n",
      "2024-11-11 06:31:51.286704: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801300 of size 512 next 19\n",
      "2024-11-11 06:31:51.286711: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801500 of size 512 next 20\n",
      "2024-11-11 06:31:51.286778: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801700 of size 1024 next 22\n",
      "2024-11-11 06:31:51.286786: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801b00 of size 256 next 23\n",
      "2024-11-11 06:31:51.286793: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801c00 of size 256 next 24\n",
      "2024-11-11 06:31:51.286800: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137801d00 of size 1024 next 26\n",
      "2024-11-11 06:31:51.286808: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137802100 of size 1024 next 29\n",
      "2024-11-11 06:31:51.286815: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137802500 of size 1024 next 30\n",
      "2024-11-11 06:31:51.286822: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137802900 of size 1024 next 114\n",
      "2024-11-11 06:31:51.286829: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137802d00 of size 1024 next 32\n",
      "2024-11-11 06:31:51.286836: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137803100 of size 256 next 33\n",
      "2024-11-11 06:31:51.286844: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137803200 of size 256 next 34\n",
      "2024-11-11 06:31:51.286851: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137803300 of size 1024 next 37\n",
      "2024-11-11 06:31:51.286858: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137803700 of size 1024 next 38\n",
      "2024-11-11 06:31:51.286865: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137803b00 of size 1024 next 39\n",
      "2024-11-11 06:31:51.286873: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0137803f00 of size 2048 next 41\n",
      "2024-11-11 06:31:51.286880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137804700 of size 256 next 42\n",
      "2024-11-11 06:31:51.286887: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137804800 of size 256 next 43\n",
      "2024-11-11 06:31:51.286895: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137804900 of size 2048 next 45\n",
      "2024-11-11 06:31:51.286902: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805100 of size 256 next 48\n",
      "2024-11-11 06:31:51.286909: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805200 of size 256 next 49\n",
      "2024-11-11 06:31:51.286916: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805300 of size 256 next 50\n",
      "2024-11-11 06:31:51.286923: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805400 of size 256 next 51\n",
      "2024-11-11 06:31:51.286931: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805500 of size 256 next 53\n",
      "2024-11-11 06:31:51.286938: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805600 of size 256 next 54\n",
      "2024-11-11 06:31:51.286945: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805700 of size 256 next 56\n",
      "2024-11-11 06:31:51.286952: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805800 of size 256 next 57\n",
      "2024-11-11 06:31:51.286959: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805900 of size 256 next 58\n",
      "2024-11-11 06:31:51.286967: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805a00 of size 256 next 59\n",
      "2024-11-11 06:31:51.286974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805b00 of size 256 next 60\n",
      "2024-11-11 06:31:51.286981: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805c00 of size 256 next 62\n",
      "2024-11-11 06:31:51.286989: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805d00 of size 256 next 63\n",
      "2024-11-11 06:31:51.286995: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805e00 of size 256 next 64\n",
      "2024-11-11 06:31:51.287003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137805f00 of size 512 next 66\n",
      "2024-11-11 06:31:51.287376: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137806100 of size 512 next 67\n",
      "2024-11-11 06:31:51.287384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137806300 of size 512 next 68\n",
      "2024-11-11 06:31:51.287392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137806500 of size 1024 next 70\n",
      "2024-11-11 06:31:51.287399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137806900 of size 1024 next 71\n",
      "2024-11-11 06:31:51.287406: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137806d00 of size 1024 next 72\n",
      "2024-11-11 06:31:51.287413: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137807100 of size 1024 next 73\n",
      "2024-11-11 06:31:51.287421: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137807500 of size 1024 next 74\n",
      "2024-11-11 06:31:51.287428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137807900 of size 1024 next 75\n",
      "2024-11-11 06:31:51.287435: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137807d00 of size 2048 next 76\n",
      "2024-11-11 06:31:51.287442: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808500 of size 256 next 78\n",
      "2024-11-11 06:31:51.287449: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808600 of size 256 next 79\n",
      "2024-11-11 06:31:51.287457: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808700 of size 256 next 80\n",
      "2024-11-11 06:31:51.287464: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808800 of size 256 next 81\n",
      "2024-11-11 06:31:51.287471: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808900 of size 256 next 82\n",
      "2024-11-11 06:31:51.287478: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808a00 of size 256 next 83\n",
      "2024-11-11 06:31:51.287485: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808b00 of size 256 next 84\n",
      "2024-11-11 06:31:51.287492: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808c00 of size 256 next 85\n",
      "2024-11-11 06:31:51.287500: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808d00 of size 256 next 86\n",
      "2024-11-11 06:31:51.287507: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137808e00 of size 256 next 87\n",
      "2024-11-11 06:31:51.287514: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0137808f00 of size 256 next 124\n",
      "2024-11-11 06:31:51.287522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809000 of size 256 next 127\n",
      "2024-11-11 06:31:51.287529: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809100 of size 256 next 126\n",
      "2024-11-11 06:31:51.287536: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809200 of size 256 next 123\n",
      "2024-11-11 06:31:51.287543: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0137809300 of size 256 next 88\n",
      "2024-11-11 06:31:51.287551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809400 of size 256 next 90\n",
      "2024-11-11 06:31:51.287558: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0137809500 of size 768 next 92\n",
      "2024-11-11 06:31:51.287565: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809800 of size 256 next 97\n",
      "2024-11-11 06:31:51.287572: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809900 of size 256 next 98\n",
      "2024-11-11 06:31:51.287579: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809a00 of size 256 next 99\n",
      "2024-11-11 06:31:51.287587: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809b00 of size 512 next 105\n",
      "2024-11-11 06:31:51.287594: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809d00 of size 512 next 106\n",
      "2024-11-11 06:31:51.287601: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137809f00 of size 512 next 107\n",
      "2024-11-11 06:31:51.287980: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f013780a100 of size 1024 next 111\n",
      "2024-11-11 06:31:51.287989: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f013780a500 of size 1536 next 7\n",
      "2024-11-11 06:31:51.287997: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f013780ab00 of size 20736 next 8\n",
      "2024-11-11 06:31:51.288005: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f013780fc00 of size 10240 next 52\n",
      "2024-11-11 06:31:51.288012: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137812400 of size 20736 next 61\n",
      "2024-11-11 06:31:51.288020: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137817500 of size 884736 next 65\n",
      "2024-11-11 06:31:51.288027: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f01378ef500 of size 10240 next 77\n",
      "2024-11-11 06:31:51.288034: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f01378f1d00 of size 1106688 next 18446744073709551615\n",
      "2024-11-11 06:31:51.288042: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4194304\n",
      "2024-11-11 06:31:51.288049: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0137a00000 of size 884736 next 18\n",
      "2024-11-11 06:31:51.288057: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0137ad8000 of size 3309568 next 18446744073709551615\n",
      "2024-11-11 06:31:51.288064: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2024-11-11 06:31:51.288074: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 47 Chunks of size 256 totalling 11.8KiB\n",
      "2024-11-11 06:31:51.288083: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 10 Chunks of size 512 totalling 5.0KiB\n",
      "2024-11-11 06:31:51.288092: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 16 Chunks of size 1024 totalling 16.0KiB\n",
      "2024-11-11 06:31:51.288100: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-11-11 06:31:51.288108: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2048 totalling 4.0KiB\n",
      "2024-11-11 06:31:51.288116: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 10240 totalling 20.0KiB\n",
      "2024-11-11 06:31:51.288124: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 20736 totalling 40.5KiB\n",
      "2024-11-11 06:31:51.288132: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 884736 totalling 1.69MiB\n",
      "2024-11-11 06:31:51.288140: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 3538944 totalling 6.75MiB\n",
      "2024-11-11 06:31:51.288148: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7077888 totalling 6.75MiB\n",
      "2024-11-11 06:31:51.288156: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 13238272 totalling 12.62MiB\n",
      "2024-11-11 06:31:51.288165: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 62914560 totalling 60.00MiB\n",
      "2024-11-11 06:31:51.288173: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 71303168 totalling 68.00MiB\n",
      "2024-11-11 06:31:51.288182: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 157843456 totalling 150.53MiB\n",
      "2024-11-11 06:31:51.288190: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 176947200 totalling 337.50MiB\n",
      "2024-11-11 06:31:51.288198: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 589824000 totalling 1.10GiB\n",
      "2024-11-11 06:31:51.288206: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 1179648000 totalling 4.39GiB\n",
      "2024-11-11 06:31:51.288214: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 2359296000 totalling 6.59GiB\n",
      "2024-11-11 06:31:51.288222: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 12.71GiB\n",
      "2024-11-11 06:31:51.288230: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 15519580160 memory_limit_: 15519580160 available bytes: 0 curr_region_allocation_bytes_: 17179869184\n",
      "2024-11-11 06:31:51.288468: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                     15519580160\n",
      "InUse:                     13651347968\n",
      "MaxInUse:                  14987203584\n",
      "NumAllocs:                         285\n",
      "MaxAllocSize:               2376073216\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-11-11 06:31:51.288486: W tensorflow/core/common_runtime/bfc_allocator.cc:475] ***************************************__**************************_************************_____***\n",
      "2024-11-11 06:31:51.288562: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_pooling_gpu.cc:153 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[64,128,5,60,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[64,128,5,60,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1426]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad:\nIn[0] sequential/activation_1/Elu (defined at /usr/local/lib/python3.8/dist-packages/keras/backend.py:4893)\t\nIn[1] sequential/max_pooling3d_1/MaxPool3D (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/pooling.py:699)\t\nIn[2] gradient_tape/sequential/conv3d_2/Conv3D/Conv3DBackpropInputV2:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_233/3678496989.py\", line 1, in <module>\n>>>     conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_233/3678496989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,128,5,60,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1426]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad:\nIn[0] sequential/activation_1/Elu (defined at /usr/local/lib/python3.8/dist-packages/keras/backend.py:4893)\t\nIn[1] sequential/max_pooling3d_1/MaxPool3D (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/pooling.py:699)\t\nIn[2] gradient_tape/sequential/conv3d_2/Conv3D/Conv3DBackpropInputV2:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_233/3678496989.py\", line 1, in <module>\n>>>     conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> "
     ]
    }
   ],
   "source": [
    "conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 1 is giving the out of memory error with batch size 64. We try with less batch size and shapes to further improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_2\">Model 2:</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 6, 50, 50, 64)     5248      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 6, 50, 50, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6, 50, 50, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 3, 25, 50, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 3, 25, 50, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 3, 25, 50, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 3, 25, 50, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 2, 13, 25, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 2, 13, 25, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 2, 13, 25, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 2, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 1, 7, 13, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 1, 7, 13, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 1, 7, 13, 256)    1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1, 7, 13, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 4, 7, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               3670528   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,557,189\n",
      "Trainable params: 6,555,781\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2: No of Epochs = 20; batch_size = 20; shape = (50,50); no of frames = 6\n",
    "\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,5)),50,50,20,20)\n",
    "conv_model2=Conv3DModel()\n",
    "conv_model2=conv_model2.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.5910 - categorical_accuracy: 0.3206Source path =  /datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.23292, saving model to model_init_2024-11-1106_31_00.946146/model-00001-2.59100-0.32059-8.23292-0.23000.h5\n",
      "34/34 [==============================] - 43s 1s/step - loss: 2.5910 - categorical_accuracy: 0.3206 - val_loss: 8.2329 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6697 - categorical_accuracy: 0.4735\n",
      "Epoch 00002: val_loss improved from 8.23292 to 5.50524, saving model to model_init_2024-11-1106_31_00.946146/model-00002-1.66967-0.47353-5.50524-0.25000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.6697 - categorical_accuracy: 0.4735 - val_loss: 5.5052 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3501 - categorical_accuracy: 0.5500\n",
      "Epoch 00003: val_loss improved from 5.50524 to 2.76308, saving model to model_init_2024-11-1106_31_00.946146/model-00003-1.35011-0.55000-2.76308-0.35000.h5\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.3501 - categorical_accuracy: 0.5500 - val_loss: 2.7631 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3240 - categorical_accuracy: 0.5676\n",
      "Epoch 00004: val_loss improved from 2.76308 to 1.60905, saving model to model_init_2024-11-1106_31_00.946146/model-00004-1.32403-0.56765-1.60905-0.47000.h5\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.3240 - categorical_accuracy: 0.5676 - val_loss: 1.6091 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0745 - categorical_accuracy: 0.6324\n",
      "Epoch 00005: val_loss improved from 1.60905 to 1.21504, saving model to model_init_2024-11-1106_31_00.946146/model-00005-1.07446-0.63235-1.21504-0.61000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.0745 - categorical_accuracy: 0.6324 - val_loss: 1.2150 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9041 - categorical_accuracy: 0.6632\n",
      "Epoch 00006: val_loss improved from 1.21504 to 0.91933, saving model to model_init_2024-11-1106_31_00.946146/model-00006-0.90413-0.66324-0.91933-0.63000.h5\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.9041 - categorical_accuracy: 0.6632 - val_loss: 0.9193 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8853 - categorical_accuracy: 0.6985\n",
      "Epoch 00007: val_loss improved from 0.91933 to 0.85328, saving model to model_init_2024-11-1106_31_00.946146/model-00007-0.88532-0.69853-0.85328-0.65000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.8853 - categorical_accuracy: 0.6985 - val_loss: 0.8533 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7974 - categorical_accuracy: 0.7265\n",
      "Epoch 00008: val_loss did not improve from 0.85328\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.7974 - categorical_accuracy: 0.7265 - val_loss: 0.9270 - val_categorical_accuracy: 0.6600 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7369 - categorical_accuracy: 0.7338\n",
      "Epoch 00009: val_loss improved from 0.85328 to 0.84919, saving model to model_init_2024-11-1106_31_00.946146/model-00009-0.73689-0.73382-0.84919-0.69000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.7369 - categorical_accuracy: 0.7338 - val_loss: 0.8492 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6773 - categorical_accuracy: 0.7397\n",
      "Epoch 00010: val_loss improved from 0.84919 to 0.83445, saving model to model_init_2024-11-1106_31_00.946146/model-00010-0.67735-0.73971-0.83445-0.67000.h5\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.6773 - categorical_accuracy: 0.7397 - val_loss: 0.8345 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5911 - categorical_accuracy: 0.7721\n",
      "Epoch 00011: val_loss did not improve from 0.83445\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.5911 - categorical_accuracy: 0.7721 - val_loss: 0.8465 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5432 - categorical_accuracy: 0.8074\n",
      "Epoch 00012: val_loss improved from 0.83445 to 0.80139, saving model to model_init_2024-11-1106_31_00.946146/model-00012-0.54324-0.80735-0.80139-0.67000.h5\n",
      "34/34 [==============================] - 39s 1s/step - loss: 0.5432 - categorical_accuracy: 0.8074 - val_loss: 0.8014 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5574 - categorical_accuracy: 0.7868\n",
      "Epoch 00013: val_loss did not improve from 0.80139\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.5574 - categorical_accuracy: 0.7868 - val_loss: 0.8612 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4493 - categorical_accuracy: 0.8250\n",
      "Epoch 00014: val_loss improved from 0.80139 to 0.69328, saving model to model_init_2024-11-1106_31_00.946146/model-00014-0.44930-0.82500-0.69328-0.76000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.4493 - categorical_accuracy: 0.8250 - val_loss: 0.6933 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4130 - categorical_accuracy: 0.8441\n",
      "Epoch 00015: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.4130 - categorical_accuracy: 0.8441 - val_loss: 1.1876 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4269 - categorical_accuracy: 0.8471\n",
      "Epoch 00016: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.4269 - categorical_accuracy: 0.8471 - val_loss: 0.8195 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3615 - categorical_accuracy: 0.8632\n",
      "Epoch 00017: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.3615 - categorical_accuracy: 0.8632 - val_loss: 0.7340 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2785 - categorical_accuracy: 0.8985\n",
      "Epoch 00018: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.2785 - categorical_accuracy: 0.8985 - val_loss: 0.9758 - val_categorical_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2967 - categorical_accuracy: 0.8941\n",
      "Epoch 00019: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.2967 - categorical_accuracy: 0.8941 - val_loss: 0.7707 - val_categorical_accuracy: 0.8200 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2331 - categorical_accuracy: 0.9221\n",
      "Epoch 00020: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.2331 - categorical_accuracy: 0.9221 - val_loss: 0.8207 - val_categorical_accuracy: 0.8400 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0210507d30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    - Number of Epochs =20; Batch size=20; Number of frames=6\n",
    "    - Taking the Frames with the step size 5 and taking 6 frames with shape (50,50) have increased the performance tremendously for both the training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_3\">Model 3: </a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_8 (Conv3D)           (None, 10, 50, 50, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 10, 50, 50, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 10, 50, 50, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 5, 25, 50, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 5, 25, 50, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 5, 25, 50, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 5, 25, 50, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 3, 13, 25, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 3, 13, 25, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 3, 13, 25, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 3, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_10 (MaxPoolin  (None, 2, 7, 13, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 2, 7, 13, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 2, 7, 13, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2, 7, 13, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_11 (MaxPoolin  (None, 1, 4, 7, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               3670528   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,557,189\n",
      "Trainable params: 6,555,781\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 20; batch_size = 30; shape = (50,50); no of frames = 10 \n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)\n",
    "conv_model3=Conv3DModel()\n",
    "conv_model3=conv_model3.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 3.2443 - categorical_accuracy: 0.3162Source path =  /datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 67s 2s/step - loss: 3.2443 - categorical_accuracy: 0.3162 - val_loss: 8.9463 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6988 - categorical_accuracy: 0.4794\n",
      "Epoch 00002: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.6988 - categorical_accuracy: 0.4794 - val_loss: 7.2703 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5891 - categorical_accuracy: 0.4956\n",
      "Epoch 00003: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.5891 - categorical_accuracy: 0.4956 - val_loss: 3.2892 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2680 - categorical_accuracy: 0.5515\n",
      "Epoch 00004: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 68s 2s/step - loss: 1.2680 - categorical_accuracy: 0.5515 - val_loss: 2.0512 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2024 - categorical_accuracy: 0.5971\n",
      "Epoch 00005: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 67s 2s/step - loss: 1.2024 - categorical_accuracy: 0.5971 - val_loss: 1.3284 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9961 - categorical_accuracy: 0.6426\n",
      "Epoch 00006: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 68s 2s/step - loss: 0.9961 - categorical_accuracy: 0.6426 - val_loss: 1.3152 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9422 - categorical_accuracy: 0.6529\n",
      "Epoch 00007: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 68s 2s/step - loss: 0.9422 - categorical_accuracy: 0.6529 - val_loss: 1.0217 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9199 - categorical_accuracy: 0.6691\n",
      "Epoch 00008: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 73s 2s/step - loss: 0.9199 - categorical_accuracy: 0.6691 - val_loss: 0.8149 - val_categorical_accuracy: 0.6400 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8655 - categorical_accuracy: 0.6882\n",
      "Epoch 00009: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.8655 - categorical_accuracy: 0.6882 - val_loss: 0.8264 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7873 - categorical_accuracy: 0.7088\n",
      "Epoch 00010: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 66s 2s/step - loss: 0.7873 - categorical_accuracy: 0.7088 - val_loss: 0.9237 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7286 - categorical_accuracy: 0.7471\n",
      "Epoch 00011: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 68s 2s/step - loss: 0.7286 - categorical_accuracy: 0.7471 - val_loss: 0.7992 - val_categorical_accuracy: 0.7000 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6438 - categorical_accuracy: 0.7691\n",
      "Epoch 00012: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 71s 2s/step - loss: 0.6438 - categorical_accuracy: 0.7691 - val_loss: 0.7044 - val_categorical_accuracy: 0.7300 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6341 - categorical_accuracy: 0.7676\n",
      "Epoch 00013: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 69s 2s/step - loss: 0.6341 - categorical_accuracy: 0.7676 - val_loss: 0.9326 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6514 - categorical_accuracy: 0.7471\n",
      "Epoch 00014: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 69s 2s/step - loss: 0.6514 - categorical_accuracy: 0.7471 - val_loss: 0.9230 - val_categorical_accuracy: 0.7000 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5934 - categorical_accuracy: 0.7706\n",
      "Epoch 00015: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 67s 2s/step - loss: 0.5934 - categorical_accuracy: 0.7706 - val_loss: 0.7065 - val_categorical_accuracy: 0.7300 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4912 - categorical_accuracy: 0.8265\n",
      "Epoch 00016: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 71s 2s/step - loss: 0.4912 - categorical_accuracy: 0.8265 - val_loss: 0.7759 - val_categorical_accuracy: 0.7300 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4900 - categorical_accuracy: 0.8059\n",
      "Epoch 00017: val_loss did not improve from 0.69328\n",
      "34/34 [==============================] - 71s 2s/step - loss: 0.4900 - categorical_accuracy: 0.8059 - val_loss: 0.7513 - val_categorical_accuracy: 0.7400 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5412 - categorical_accuracy: 0.7912\n",
      "Epoch 00018: val_loss did not improve from 0.69328\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "34/34 [==============================] - 68s 2s/step - loss: 0.5412 - categorical_accuracy: 0.7912 - val_loss: 0.9015 - val_categorical_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4897 - categorical_accuracy: 0.8176\n",
      "Epoch 00019: val_loss improved from 0.69328 to 0.60749, saving model to model_init_2024-11-1106_31_00.946146/model-00019-0.48969-0.81765-0.60749-0.81000.h5\n",
      "34/34 [==============================] - 69s 2s/step - loss: 0.4897 - categorical_accuracy: 0.8176 - val_loss: 0.6075 - val_categorical_accuracy: 0.8100 - lr: 6.2500e-05\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4848 - categorical_accuracy: 0.8132\n",
      "Epoch 00020: val_loss did not improve from 0.60749\n",
      "34/34 [==============================] - 70s 2s/step - loss: 0.4848 - categorical_accuracy: 0.8132 - val_loss: 0.9187 - val_categorical_accuracy: 0.6500 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0233b929d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 3: Number of Epochs =20; Batch size=30; shape = (50,50); Number of frames=10\n",
    "    Keeping the same shape and increasing the number of frames we have observed that Validation Accuracy decreased and slightly seems to be overfitting as compared to Model-2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_4\">Model 4: </a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 12\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_12 (Conv3D)          (None, 12, 100, 100, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 12, 100, 100, 64)  256      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 12, 100, 100, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d_12 (MaxPoolin  (None, 6, 50, 100, 64)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 6, 50, 100, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 6, 50, 100, 128)  512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 6, 50, 100, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_13 (MaxPoolin  (None, 3, 25, 50, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 3, 25, 50, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 3, 25, 50, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 3, 25, 50, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_14 (MaxPoolin  (None, 2, 13, 25, 256)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 2, 13, 25, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 2, 13, 25, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_15 (MaxPoolin  (None, 1, 7, 13, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 23296)             0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 23296)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               11928064  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,814,725\n",
      "Trainable params: 14,813,317\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 25 , batch_size = 50 ,shape = (100,100) , no of frames = 10\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(5,28,2)),100,100,50,25)\n",
    "conv_model4=Conv3DModel()\n",
    "conv_model4=conv_model4.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 4.1682 - categorical_accuracy: 0.3314Source path =  /datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 77s 5s/step - loss: 4.1682 - categorical_accuracy: 0.3314 - val_loss: 9.7179 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5756 - categorical_accuracy: 0.5071\n",
      "Epoch 00002: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 73s 6s/step - loss: 1.5756 - categorical_accuracy: 0.5071 - val_loss: 7.4664 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1954 - categorical_accuracy: 0.6043\n",
      "Epoch 00003: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 74s 6s/step - loss: 1.1954 - categorical_accuracy: 0.6043 - val_loss: 5.8911 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0962 - categorical_accuracy: 0.6029\n",
      "Epoch 00004: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 75s 6s/step - loss: 1.0962 - categorical_accuracy: 0.6029 - val_loss: 5.5201 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0777 - categorical_accuracy: 0.6343\n",
      "Epoch 00005: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 77s 6s/step - loss: 1.0777 - categorical_accuracy: 0.6343 - val_loss: 4.3320 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8668 - categorical_accuracy: 0.6871\n",
      "Epoch 00006: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 75s 6s/step - loss: 0.8668 - categorical_accuracy: 0.6871 - val_loss: 3.1596 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8042 - categorical_accuracy: 0.7143\n",
      "Epoch 00007: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 75s 6s/step - loss: 0.8042 - categorical_accuracy: 0.7143 - val_loss: 2.0766 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7119 - categorical_accuracy: 0.7471\n",
      "Epoch 00008: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 74s 6s/step - loss: 0.7119 - categorical_accuracy: 0.7471 - val_loss: 2.4051 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6607 - categorical_accuracy: 0.7686\n",
      "Epoch 00009: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 75s 6s/step - loss: 0.6607 - categorical_accuracy: 0.7686 - val_loss: 1.6165 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6796 - categorical_accuracy: 0.7529\n",
      "Epoch 00010: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 73s 6s/step - loss: 0.6796 - categorical_accuracy: 0.7529 - val_loss: 1.2935 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6080 - categorical_accuracy: 0.7757\n",
      "Epoch 00011: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 74s 6s/step - loss: 0.6080 - categorical_accuracy: 0.7757 - val_loss: 1.2891 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5915 - categorical_accuracy: 0.7871\n",
      "Epoch 00012: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 74s 6s/step - loss: 0.5915 - categorical_accuracy: 0.7871 - val_loss: 1.1559 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4704 - categorical_accuracy: 0.8200\n",
      "Epoch 00013: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 75s 6s/step - loss: 0.4704 - categorical_accuracy: 0.8200 - val_loss: 0.7782 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5443 - categorical_accuracy: 0.8086\n",
      "Epoch 00014: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 74s 6s/step - loss: 0.5443 - categorical_accuracy: 0.8086 - val_loss: 0.7327 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4403 - categorical_accuracy: 0.8300\n",
      "Epoch 00015: val_loss did not improve from 0.60749\n",
      "14/14 [==============================] - 72s 6s/step - loss: 0.4403 - categorical_accuracy: 0.8300 - val_loss: 0.6769 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4988 - categorical_accuracy: 0.8229\n",
      "Epoch 00016: val_loss improved from 0.60749 to 0.54788, saving model to model_init_2024-11-1106_31_00.946146/model-00016-0.49884-0.82286-0.54788-0.82000.h5\n",
      "14/14 [==============================] - 74s 6s/step - loss: 0.4988 - categorical_accuracy: 0.8229 - val_loss: 0.5479 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4122 - categorical_accuracy: 0.8500\n",
      "Epoch 00017: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 72s 5s/step - loss: 0.4122 - categorical_accuracy: 0.8500 - val_loss: 0.8208 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3596 - categorical_accuracy: 0.8800\n",
      "Epoch 00018: val_loss did not improve from 0.54788\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 72s 6s/step - loss: 0.3596 - categorical_accuracy: 0.8800 - val_loss: 0.9447 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3509 - categorical_accuracy: 0.8714\n",
      "Epoch 00019: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 74s 6s/step - loss: 0.3509 - categorical_accuracy: 0.8714 - val_loss: 0.7562 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 20/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3306 - categorical_accuracy: 0.8857\n",
      "Epoch 00020: val_loss did not improve from 0.54788\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 71s 5s/step - loss: 0.3306 - categorical_accuracy: 0.8857 - val_loss: 0.6154 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 21/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2574 - categorical_accuracy: 0.9057\n",
      "Epoch 00021: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 76s 6s/step - loss: 0.2574 - categorical_accuracy: 0.9057 - val_loss: 0.6982 - val_categorical_accuracy: 0.7600 - lr: 2.5000e-04\n",
      "Epoch 22/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2511 - categorical_accuracy: 0.9057\n",
      "Epoch 00022: val_loss did not improve from 0.54788\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 75s 6s/step - loss: 0.2511 - categorical_accuracy: 0.9057 - val_loss: 0.7792 - val_categorical_accuracy: 0.8000 - lr: 2.5000e-04\n",
      "Epoch 23/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2586 - categorical_accuracy: 0.8986\n",
      "Epoch 00023: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 72s 5s/step - loss: 0.2586 - categorical_accuracy: 0.8986 - val_loss: 0.6599 - val_categorical_accuracy: 0.7800 - lr: 1.2500e-04\n",
      "Epoch 24/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2517 - categorical_accuracy: 0.9014\n",
      "Epoch 00024: val_loss did not improve from 0.54788\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 73s 6s/step - loss: 0.2517 - categorical_accuracy: 0.9014 - val_loss: 0.6312 - val_categorical_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 25/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2081 - categorical_accuracy: 0.9086\n",
      "Epoch 00025: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 73s 6s/step - loss: 0.2081 - categorical_accuracy: 0.9086 - val_loss: 0.6355 - val_categorical_accuracy: 0.7800 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f02339b5ee0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "Model 4: This model seems to be overfitting. Increasing the image size decreases the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_5\">Model 5: </a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_16 (Conv3D)          (None, 18, 70, 70, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 18, 70, 70, 64)   256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 18, 70, 70, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPoolin  (None, 9, 35, 70, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 9, 35, 70, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 9, 35, 70, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 9, 35, 70, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 5, 18, 35, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 5, 18, 35, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 5, 18, 35, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 5, 18, 35, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 9, 18, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 3, 9, 18, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 3, 9, 18, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 3, 9, 18, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 2, 5, 9, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 23040)             0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 23040)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               11796992  \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,683,653\n",
      "Trainable params: 14,682,245\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 25 , batch_size = 50 ,shape = (70,70) , no of frames = 18 \n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],70,70,50,34)\n",
    "conv_model5=Conv3DModel()\n",
    "conv_model5=conv_model5.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model5.summary()\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 3.5425 - categorical_accuracy: 0.3271Source path =  /datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 115s 8s/step - loss: 3.5425 - categorical_accuracy: 0.3271 - val_loss: 11.5340 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6635 - categorical_accuracy: 0.5171\n",
      "Epoch 00002: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 114s 9s/step - loss: 1.6635 - categorical_accuracy: 0.5171 - val_loss: 8.0788 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3348 - categorical_accuracy: 0.6014\n",
      "Epoch 00003: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 112s 9s/step - loss: 1.3348 - categorical_accuracy: 0.6014 - val_loss: 7.7256 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0885 - categorical_accuracy: 0.6100\n",
      "Epoch 00004: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 113s 9s/step - loss: 1.0885 - categorical_accuracy: 0.6100 - val_loss: 5.4247 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8875 - categorical_accuracy: 0.6771\n",
      "Epoch 00005: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 112s 9s/step - loss: 0.8875 - categorical_accuracy: 0.6771 - val_loss: 2.9305 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8879 - categorical_accuracy: 0.6771\n",
      "Epoch 00006: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 111s 8s/step - loss: 0.8879 - categorical_accuracy: 0.6771 - val_loss: 3.1175 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7970 - categorical_accuracy: 0.7129\n",
      "Epoch 00007: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 113s 9s/step - loss: 0.7970 - categorical_accuracy: 0.7129 - val_loss: 2.0963 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8091 - categorical_accuracy: 0.7271\n",
      "Epoch 00008: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 112s 9s/step - loss: 0.8091 - categorical_accuracy: 0.7271 - val_loss: 1.8678 - val_categorical_accuracy: 0.4100 - lr: 0.0010\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6548 - categorical_accuracy: 0.7643\n",
      "Epoch 00009: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 112s 9s/step - loss: 0.6548 - categorical_accuracy: 0.7643 - val_loss: 1.8061 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6071 - categorical_accuracy: 0.7614\n",
      "Epoch 00010: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 116s 9s/step - loss: 0.6071 - categorical_accuracy: 0.7614 - val_loss: 1.3182 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5938 - categorical_accuracy: 0.7729\n",
      "Epoch 00011: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 115s 9s/step - loss: 0.5938 - categorical_accuracy: 0.7729 - val_loss: 0.9491 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5833 - categorical_accuracy: 0.7929\n",
      "Epoch 00012: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 116s 9s/step - loss: 0.5833 - categorical_accuracy: 0.7929 - val_loss: 0.7810 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4344 - categorical_accuracy: 0.8429\n",
      "Epoch 00013: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 150s 11s/step - loss: 0.4344 - categorical_accuracy: 0.8429 - val_loss: 0.8019 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4184 - categorical_accuracy: 0.8329\n",
      "Epoch 00014: val_loss did not improve from 0.54788\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 109s 8s/step - loss: 0.4184 - categorical_accuracy: 0.8329 - val_loss: 0.7959 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3943 - categorical_accuracy: 0.8429\n",
      "Epoch 00015: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.3943 - categorical_accuracy: 0.8429 - val_loss: 0.6678 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3616 - categorical_accuracy: 0.8614\n",
      "Epoch 00016: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.3616 - categorical_accuracy: 0.8614 - val_loss: 0.6892 - val_categorical_accuracy: 0.7300 - lr: 5.0000e-04\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3371 - categorical_accuracy: 0.8686\n",
      "Epoch 00017: val_loss did not improve from 0.54788\n",
      "14/14 [==============================] - 104s 8s/step - loss: 0.3371 - categorical_accuracy: 0.8686 - val_loss: 0.6313 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3099 - categorical_accuracy: 0.8757\n",
      "Epoch 00018: val_loss improved from 0.54788 to 0.47978, saving model to model_init_2024-11-1106_31_00.946146/model-00018-0.30986-0.87571-0.47978-0.83000.h5\n",
      "14/14 [==============================] - 105s 8s/step - loss: 0.3099 - categorical_accuracy: 0.8757 - val_loss: 0.4798 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2700 - categorical_accuracy: 0.8943\n",
      "Epoch 00019: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.2700 - categorical_accuracy: 0.8943 - val_loss: 0.5947 - val_categorical_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2786 - categorical_accuracy: 0.8914\n",
      "Epoch 00020: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 105s 8s/step - loss: 0.2786 - categorical_accuracy: 0.8914 - val_loss: 0.6212 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2430 - categorical_accuracy: 0.9043\n",
      "Epoch 00021: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 108s 8s/step - loss: 0.2430 - categorical_accuracy: 0.9043 - val_loss: 0.6504 - val_categorical_accuracy: 0.7300 - lr: 2.5000e-04\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2465 - categorical_accuracy: 0.9057\n",
      "Epoch 00022: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 105s 8s/step - loss: 0.2465 - categorical_accuracy: 0.9057 - val_loss: 0.6332 - val_categorical_accuracy: 0.7300 - lr: 2.5000e-04\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2359 - categorical_accuracy: 0.9029\n",
      "Epoch 00023: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 105s 8s/step - loss: 0.2359 - categorical_accuracy: 0.9029 - val_loss: 0.6424 - val_categorical_accuracy: 0.7400 - lr: 1.2500e-04\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2000 - categorical_accuracy: 0.9200\n",
      "Epoch 00024: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.2000 - categorical_accuracy: 0.9200 - val_loss: 0.6101 - val_categorical_accuracy: 0.7800 - lr: 1.2500e-04\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1892 - categorical_accuracy: 0.9314\n",
      "Epoch 00025: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1892 - categorical_accuracy: 0.9314 - val_loss: 0.6310 - val_categorical_accuracy: 0.7600 - lr: 6.2500e-05\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2160 - categorical_accuracy: 0.9271\n",
      "Epoch 00026: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 106s 8s/step - loss: 0.2160 - categorical_accuracy: 0.9271 - val_loss: 0.6166 - val_categorical_accuracy: 0.7900 - lr: 6.2500e-05\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2210 - categorical_accuracy: 0.9186\n",
      "Epoch 00027: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.2210 - categorical_accuracy: 0.9186 - val_loss: 0.6234 - val_categorical_accuracy: 0.7700 - lr: 3.1250e-05\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2248 - categorical_accuracy: 0.9057\n",
      "Epoch 00028: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.2248 - categorical_accuracy: 0.9057 - val_loss: 0.5564 - val_categorical_accuracy: 0.8200 - lr: 3.1250e-05\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2139 - categorical_accuracy: 0.9129\n",
      "Epoch 00029: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.2139 - categorical_accuracy: 0.9129 - val_loss: 0.6224 - val_categorical_accuracy: 0.7600 - lr: 1.5625e-05\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2407 - categorical_accuracy: 0.9043\n",
      "Epoch 00030: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.2407 - categorical_accuracy: 0.9043 - val_loss: 0.6230 - val_categorical_accuracy: 0.7900 - lr: 1.5625e-05\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2034 - categorical_accuracy: 0.9271\n",
      "Epoch 00031: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.2034 - categorical_accuracy: 0.9271 - val_loss: 0.6124 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2056 - categorical_accuracy: 0.9286\n",
      "Epoch 00032: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.2056 - categorical_accuracy: 0.9286 - val_loss: 0.6225 - val_categorical_accuracy: 0.7800 - lr: 1.0000e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2095 - categorical_accuracy: 0.9200\n",
      "Epoch 00033: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.2095 - categorical_accuracy: 0.9200 - val_loss: 0.6102 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2175 - categorical_accuracy: 0.9286\n",
      "Epoch 00034: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.2175 - categorical_accuracy: 0.9286 - val_loss: 0.5835 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f02337428b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 5 is clearly an overfit model can see that increasing in number of frames and epochs causing the noise to be learned also from all the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Insights for Model 1 to 5:\n",
    "    Based on our experiment the final model will be model 2 - Less no of frames and reducing image size to 50,50 giving good results\n",
    "    Model 2 No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_6\">Model 6 <br></a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking image_height and image_width as 70,70 , batch size 50 and no of epochs 25\n",
    "#Switching Model architecture to Conv2D+LSTM\n",
    "# Conv2D_18, 70, 70, 16\n",
    "# LSTM_512\n",
    "# Dense_512_5\n",
    "\n",
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(512),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_2d_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_2d_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 18, 70, 70, 16)   448       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 18, 70, 70, 16)   64        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 18, 35, 35, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 18, 35, 35, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 18, 35, 35, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 18, 17, 17, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 18, 17, 17, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 18, 17, 17, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 18, 8, 8, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 18, 8, 8, 128)    73856     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 18, 8, 8, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 18, 4, 4, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 18, 4, 4, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 18, 4, 4, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 18, 2, 2, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               3147776   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,807,589\n",
      "Trainable params: 3,806,597\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, 20)\n",
    "val_generator = generator(val_path, val_doc, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6666 - categorical_accuracy: 0.1964Source path =  /datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.6666 - categorical_accuracy: 0.1964 - val_loss: 1.6567 - val_categorical_accuracy: 0.1250 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5593 - categorical_accuracy: 0.3214\n",
      "Epoch 00002: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.5593 - categorical_accuracy: 0.3214 - val_loss: 1.6502 - val_categorical_accuracy: 0.1750 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4210 - categorical_accuracy: 0.3929\n",
      "Epoch 00003: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.4210 - categorical_accuracy: 0.3929 - val_loss: 1.6190 - val_categorical_accuracy: 0.2250 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3009 - categorical_accuracy: 0.4929\n",
      "Epoch 00004: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.3009 - categorical_accuracy: 0.4929 - val_loss: 1.6657 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2543 - categorical_accuracy: 0.5250\n",
      "Epoch 00005: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.2543 - categorical_accuracy: 0.5250 - val_loss: 1.6567 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1835 - categorical_accuracy: 0.6000\n",
      "Epoch 00006: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.1835 - categorical_accuracy: 0.6000 - val_loss: 1.4754 - val_categorical_accuracy: 0.2750 - lr: 5.0000e-04\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1286 - categorical_accuracy: 0.6214\n",
      "Epoch 00007: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.1286 - categorical_accuracy: 0.6214 - val_loss: 1.5319 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-04\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0550 - categorical_accuracy: 0.6893\n",
      "Epoch 00008: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.0550 - categorical_accuracy: 0.6893 - val_loss: 1.4225 - val_categorical_accuracy: 0.3250 - lr: 5.0000e-04\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0290 - categorical_accuracy: 0.6893\n",
      "Epoch 00009: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.0290 - categorical_accuracy: 0.6893 - val_loss: 1.4134 - val_categorical_accuracy: 0.3000 - lr: 5.0000e-04\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0255 - categorical_accuracy: 0.6821\n",
      "Epoch 00010: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 38s 3s/step - loss: 1.0255 - categorical_accuracy: 0.6821 - val_loss: 1.2733 - val_categorical_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9822 - categorical_accuracy: 0.7286\n",
      "Epoch 00011: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.9822 - categorical_accuracy: 0.7286 - val_loss: 1.3904 - val_categorical_accuracy: 0.4250 - lr: 5.0000e-04\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9486 - categorical_accuracy: 0.7179\n",
      "Epoch 00012: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.9486 - categorical_accuracy: 0.7179 - val_loss: 1.1649 - val_categorical_accuracy: 0.5250 - lr: 5.0000e-04\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8966 - categorical_accuracy: 0.7679\n",
      "Epoch 00013: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.8966 - categorical_accuracy: 0.7679 - val_loss: 1.3028 - val_categorical_accuracy: 0.5500 - lr: 5.0000e-04\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8405 - categorical_accuracy: 0.7964\n",
      "Epoch 00014: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.8405 - categorical_accuracy: 0.7964 - val_loss: 1.0961 - val_categorical_accuracy: 0.6500 - lr: 5.0000e-04\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8536 - categorical_accuracy: 0.7714\n",
      "Epoch 00015: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.8536 - categorical_accuracy: 0.7714 - val_loss: 1.1476 - val_categorical_accuracy: 0.4750 - lr: 5.0000e-04\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8627 - categorical_accuracy: 0.7643\n",
      "Epoch 00016: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.8627 - categorical_accuracy: 0.7643 - val_loss: 1.1672 - val_categorical_accuracy: 0.5250 - lr: 5.0000e-04\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7909 - categorical_accuracy: 0.8286\n",
      "Epoch 00017: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.7909 - categorical_accuracy: 0.8286 - val_loss: 1.0276 - val_categorical_accuracy: 0.7000 - lr: 2.5000e-04\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7846 - categorical_accuracy: 0.8036\n",
      "Epoch 00018: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.7846 - categorical_accuracy: 0.8036 - val_loss: 1.0883 - val_categorical_accuracy: 0.6500 - lr: 2.5000e-04\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7709 - categorical_accuracy: 0.7964\n",
      "Epoch 00019: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7709 - categorical_accuracy: 0.7964 - val_loss: 1.0858 - val_categorical_accuracy: 0.6500 - lr: 2.5000e-04\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7368 - categorical_accuracy: 0.8500\n",
      "Epoch 00020: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.7368 - categorical_accuracy: 0.8500 - val_loss: 1.0071 - val_categorical_accuracy: 0.6500 - lr: 1.2500e-04\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7353 - categorical_accuracy: 0.8179\n",
      "Epoch 00021: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.7353 - categorical_accuracy: 0.8179 - val_loss: 0.9429 - val_categorical_accuracy: 0.7250 - lr: 1.2500e-04\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7646 - categorical_accuracy: 0.8214\n",
      "Epoch 00022: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 37s 3s/step - loss: 0.7646 - categorical_accuracy: 0.8214 - val_loss: 0.9967 - val_categorical_accuracy: 0.5750 - lr: 1.2500e-04\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7001 - categorical_accuracy: 0.8786\n",
      "Epoch 00023: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7001 - categorical_accuracy: 0.8786 - val_loss: 0.9091 - val_categorical_accuracy: 0.6750 - lr: 1.2500e-04\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7233 - categorical_accuracy: 0.8250\n",
      "Epoch 00024: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.7233 - categorical_accuracy: 0.8250 - val_loss: 1.0011 - val_categorical_accuracy: 0.6250 - lr: 1.2500e-04\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7304 - categorical_accuracy: 0.8321\n",
      "Epoch 00025: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7304 - categorical_accuracy: 0.8321 - val_loss: 1.0021 - val_categorical_accuracy: 0.6250 - lr: 1.2500e-04\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7153 - categorical_accuracy: 0.8464\n",
      "Epoch 00026: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.7153 - categorical_accuracy: 0.8464 - val_loss: 0.9239 - val_categorical_accuracy: 0.6500 - lr: 6.2500e-05\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6646 - categorical_accuracy: 0.8750\n",
      "Epoch 00027: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.6646 - categorical_accuracy: 0.8750 - val_loss: 1.0060 - val_categorical_accuracy: 0.6000 - lr: 6.2500e-05\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7508 - categorical_accuracy: 0.8286\n",
      "Epoch 00028: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7508 - categorical_accuracy: 0.8286 - val_loss: 0.9530 - val_categorical_accuracy: 0.6750 - lr: 3.1250e-05\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6936 - categorical_accuracy: 0.8357\n",
      "Epoch 00029: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.6936 - categorical_accuracy: 0.8357 - val_loss: 0.8792 - val_categorical_accuracy: 0.6750 - lr: 3.1250e-05\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7034 - categorical_accuracy: 0.8607\n",
      "Epoch 00030: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.7034 - categorical_accuracy: 0.8607 - val_loss: 0.9234 - val_categorical_accuracy: 0.6750 - lr: 3.1250e-05\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6828 - categorical_accuracy: 0.8571\n",
      "Epoch 00031: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6828 - categorical_accuracy: 0.8571 - val_loss: 1.0567 - val_categorical_accuracy: 0.5500 - lr: 3.1250e-05\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6985 - categorical_accuracy: 0.8500\n",
      "Epoch 00032: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.6985 - categorical_accuracy: 0.8500 - val_loss: 0.7640 - val_categorical_accuracy: 0.7500 - lr: 1.5625e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6855 - categorical_accuracy: 0.8821\n",
      "Epoch 00033: val_loss did not improve from 0.47978\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.6855 - categorical_accuracy: 0.8821 - val_loss: 1.0229 - val_categorical_accuracy: 0.5750 - lr: 1.5625e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7088 - categorical_accuracy: 0.8321\n",
      "Epoch 00034: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.7088 - categorical_accuracy: 0.8321 - val_loss: 1.0022 - val_categorical_accuracy: 0.5750 - lr: 1.5625e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0200f74d00>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model-6 is clearly overfitting.\n",
    "    We will change the number of frames, image size and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_7\">Model 7 Using Transfer Learning - MobileNet</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],120,120,5,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "from keras.applications import mobilenet\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(TimeDistributed(mobilenet_transfer,input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_16 (TimeDi  (None, 18, 3, 3, 1024)   3228864   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 18, 3, 3, 1024)   4096      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 18, 1, 1, 1024)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               443136    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 5\n",
      "Epoch 1/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.3584 - categorical_accuracy: 0.4165Source path =  /datasets/Project_data/val ; batch size = 5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47978\n",
      "133/133 [==============================] - 99s 718ms/step - loss: 1.3584 - categorical_accuracy: 0.4165 - val_loss: 0.8169 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8722 - categorical_accuracy: 0.6436\n",
      "Epoch 00002: val_loss did not improve from 0.47978\n",
      "133/133 [==============================] - 90s 679ms/step - loss: 0.8722 - categorical_accuracy: 0.6436 - val_loss: 0.5923 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6225 - categorical_accuracy: 0.7699\n",
      "Epoch 00003: val_loss did not improve from 0.47978\n",
      "133/133 [==============================] - 90s 681ms/step - loss: 0.6225 - categorical_accuracy: 0.7699 - val_loss: 0.7648 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5633 - categorical_accuracy: 0.8015\n",
      "Epoch 00004: val_loss did not improve from 0.47978\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "133/133 [==============================] - 90s 681ms/step - loss: 0.5633 - categorical_accuracy: 0.8015 - val_loss: 1.5467 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4524 - categorical_accuracy: 0.8526\n",
      "Epoch 00005: val_loss improved from 0.47978 to 0.34739, saving model to model_init_2024-11-1106_31_00.946146/model-00005-0.45237-0.85263-0.34739-0.84000.h5\n",
      "133/133 [==============================] - 92s 694ms/step - loss: 0.4524 - categorical_accuracy: 0.8526 - val_loss: 0.3474 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2453 - categorical_accuracy: 0.9308\n",
      "Epoch 00006: val_loss improved from 0.34739 to 0.24049, saving model to model_init_2024-11-1106_31_00.946146/model-00006-0.24533-0.93083-0.24049-0.91000.h5\n",
      "133/133 [==============================] - 92s 695ms/step - loss: 0.2453 - categorical_accuracy: 0.9308 - val_loss: 0.2405 - val_categorical_accuracy: 0.9100 - lr: 5.0000e-04\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1970 - categorical_accuracy: 0.9338\n",
      "Epoch 00007: val_loss improved from 0.24049 to 0.18768, saving model to model_init_2024-11-1106_31_00.946146/model-00007-0.19703-0.93383-0.18768-0.95000.h5\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.1970 - categorical_accuracy: 0.9338 - val_loss: 0.1877 - val_categorical_accuracy: 0.9500 - lr: 5.0000e-04\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1559 - categorical_accuracy: 0.9564\n",
      "Epoch 00008: val_loss did not improve from 0.18768\n",
      "133/133 [==============================] - 89s 675ms/step - loss: 0.1559 - categorical_accuracy: 0.9564 - val_loss: 0.3345 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1721 - categorical_accuracy: 0.9368\n",
      "Epoch 00009: val_loss did not improve from 0.18768\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "133/133 [==============================] - 92s 696ms/step - loss: 0.1721 - categorical_accuracy: 0.9368 - val_loss: 0.4428 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1035 - categorical_accuracy: 0.9714\n",
      "Epoch 00010: val_loss improved from 0.18768 to 0.08497, saving model to model_init_2024-11-1106_31_00.946146/model-00010-0.10348-0.97143-0.08497-0.97000.h5\n",
      "133/133 [==============================] - 92s 694ms/step - loss: 0.1035 - categorical_accuracy: 0.9714 - val_loss: 0.0850 - val_categorical_accuracy: 0.9700 - lr: 2.5000e-04\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0706 - categorical_accuracy: 0.9789\n",
      "Epoch 00011: val_loss did not improve from 0.08497\n",
      "133/133 [==============================] - 93s 706ms/step - loss: 0.0706 - categorical_accuracy: 0.9789 - val_loss: 0.2786 - val_categorical_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0317 - categorical_accuracy: 0.9940\n",
      "Epoch 00012: val_loss improved from 0.08497 to 0.07361, saving model to model_init_2024-11-1106_31_00.946146/model-00012-0.03172-0.99398-0.07361-0.97000.h5\n",
      "133/133 [==============================] - 96s 724ms/step - loss: 0.0317 - categorical_accuracy: 0.9940 - val_loss: 0.0736 - val_categorical_accuracy: 0.9700 - lr: 2.5000e-04\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0651 - categorical_accuracy: 0.9789\n",
      "Epoch 00013: val_loss did not improve from 0.07361\n",
      "133/133 [==============================] - 91s 690ms/step - loss: 0.0651 - categorical_accuracy: 0.9789 - val_loss: 0.0995 - val_categorical_accuracy: 0.9400 - lr: 2.5000e-04\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9865\n",
      "Epoch 00014: val_loss did not improve from 0.07361\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "133/133 [==============================] - 92s 697ms/step - loss: 0.0407 - categorical_accuracy: 0.9865 - val_loss: 0.1348 - val_categorical_accuracy: 0.9100 - lr: 2.5000e-04\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0227 - categorical_accuracy: 0.9940"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Conclusion\">Conclusion</a></h2> \n",
    "\n",
    "- # Model Statistics\n",
    "\n",
    "- # Conv3D\n",
    "\n",
    "- Model 1 : No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "- - - - Model 1 is giving the out of memory error with batch size 64. We try with less batch size and shapes to further improve the performance and accuracy\n",
    "\n",
    "- Model 2 : No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
    "\n",
    "- - - - Training Accuracy : 81.76% , Validation Accuracy : 81% , \n",
    "- - - - Model Analysis : Training and validation Accuracy are good so that we can conclude that with above set of parameters model is giving good results\n",
    "\n",
    "- Model 3 : No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10\n",
    "\n",
    "- - - - Training Accuracy : 82.50% , Validation Accuracy : 76% \n",
    "- - - - Model Analysis : Keeping the same shape and increasing the number of frames we have observed that validation accuracy decreased and seems to be overfitting as compared to Model-2\n",
    "\n",
    "- Model 4 : No of Epochs = 25 , batch_size = 50 ,shape = (100,100) , no of frames = 10\n",
    "\n",
    "- - - - Training Accuracy : 82.29% , Validation Accuracy : 82% \n",
    "- - - - Model Analysis : this model performance is poor due to accuracy.\n",
    "\n",
    "- Model 5 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "\n",
    "- - - - Training Accuracy : 87.57% , Validation Accuracy : 83% \n",
    "- - - - Model Analysis : this model performance is better as compared to the model 4.\n",
    "\n",
    "- # CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
    "\n",
    "- Model 6 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "\n",
    "- - - - Training Accuracy : 85.79% , Validation Accuracy : 75% \n",
    "- - - - Model Analysis :  The model is overfitting, as indicated by higher training accuracy (85.79%) than validation accuracy (75%), suggesting poor generalization to unseen data.\n",
    "\n",
    "- # Transfer Learning Using MobileNet\n",
    "\n",
    "-  Model 7 : No of epochs = 15 , batch_size = 5 , shape  (120,120) , no of frames  = 18\n",
    "\n",
    "- - - - Training Accuracy : 99.4% , Validation Accuracy : 97% \n",
    "- - - - Model Analysis : This is so far the best model that we got with better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
